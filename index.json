[{"authors":["admin"],"categories":null,"content":"I am tenure-track assistant professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute.\nI have a deep interest in the economic implications and evolution of blockchain-based settlement in financial markets. I pursue research questions related to market fragmentation, high frequency trading and big data in financial applications. My research is thus anchored in the intersection of market microstructure, asset pricing and financial econometrics.\nI am also co-author of the book Tidy Finance with R, written with my colleagues Christoph Scheuch and Patrick Weiss.\nYou can find my current research on SSRN and arXiv.\n\rwindow.onload = function() { Calendly.initBadgeWidget({ url: 'https://calendly.com/voigtstefan', text: 'Supervision meeting needed? Arrange here!', color: '#ff1600', textColor: '#ffffff', branding: true }); }\r","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://voigtstefan.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am tenure-track assistant professor of Finance at the Department of Economics at the University in Copenhagen and a research fellow at the Danish Finance Institute.\nI have a deep interest in the economic implications and evolution of blockchain-based settlement in financial markets.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"During the spring semester 2021 I designed a new course Advanced Empirical Finance: Topics and Data Science for the master students at KU. The course aims at providing a unified coding framework in R to tackle many (probably too many) common issues in empirical finance:\n Portfolio allocation and backtesting Portfolio sorts and asset pricing tests Machine learning in empirical asset pricing Volatility estimation High frequency trading and econometrics  Along the course I collected my own R-code and curated some solutions for typical problems. Together with my colleagues Christoph Scheuch and Patrick Weiss we created an entire textbook: This book aims to lift this curtain on reproducible finance by providing a fully transparent code base for many common financial applications. We hope to inspire others in sharing their code publicly and taking part in our journey towards more reproducible research in the future.\nWe will keep on working on updating the document further in the future, but everybody is invited to take a look, make use of the code and - even better - to provide feedback on improvements or other interesting features.\nThe full book is available here, comments are very welcome.\nWe are grateful for any kind of feedback on every aspect of the book. So please get in touch with us via contact@tidy-finance.org if you spot typos, discover any issues that deserve more attention, or if you have suggestions for additional chapters and sections.\n","date":1644883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644918997,"objectID":"30b7fe65947bc83e6ade8a5eff2f7858","permalink":"https://voigtstefan.me/post/advanced-tidy-empirical-finance/","publishdate":"2022-02-15T00:00:00Z","relpermalink":"/post/advanced-tidy-empirical-finance/","section":"post","summary":"During the spring semester 2021 I designed a new course Advanced Empirical Finance: Topics and Data Science for the master students at KU. The course aims at providing a unified coding framework in R to tackle many (probably too many) common issues in empirical finance:","tags":[],"title":"Tidy Finance with R","type":"post"},{"authors":["Nikolaus Hautsch","Christoph Scheuch","Stefan Voigt"],"categories":[],"content":"This paper replaces an earlier draft titled \u0026ldquo;Limits to Arbitrage in Markets with Stochastic Settlement Latency\u0026rdquo;.\n","date":1642291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642291200,"objectID":"cc83228c6e3b9f76a3d0dee264033837","permalink":"https://voigtstefan.me/publication/preprint/trust-takes-time-limits-to-arbitrage-in-blockchain-based-markets/","publishdate":"2020-03-19T15:38:53+01:00","relpermalink":"/publication/preprint/trust-takes-time-limits-to-arbitrage-in-blockchain-based-markets/","section":"publication","summary":"A blockchain replaces central counterparties with time-consuming consensus protocols to record the transfer of ownership. This settlement latency slows down cross-exchange trading which exposes arbitrageurs to price risk. Off-chain settlement, instead, exposes arbitrageurs to costly default risks. We show with Bitcoin network and order book data that cross-exchange price differences coincide with periods of high settlement latency, asset flows chase arbitrage opportunities, and that price differences across exchanges with low default risks are smaller. Blockchain-based asset trading thus faces a dilemma: Reliable consensus protocols require time-consuming settlement latency which leads to limits to arbitrage. Circumventing such arbitrage costs is possible only by reinstalling trusted intermediation which mitigates default risks.","tags":[],"title":"Building Trust Takes Time: Limits to Arbitrage in Blockchain-Based Markets","type":"publication"},{"authors":["Stefan Voigt"],"categories":[],"content":"","date":1642291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642291200,"objectID":"5a1fe4c433530e3a17b8836f856da09f","permalink":"https://voigtstefan.me/publication/preprint/liquidity-and-price-informativeness-in-blockchain-based-markets/","publishdate":"2019-03-20T15:43:57+01:00","relpermalink":"/publication/preprint/liquidity-and-price-informativeness-in-blockchain-based-markets/","section":"publication","summary":"Blockchain-based markets impose substantial costs on cross-market trading due to the decentralized and time-consuming settlement process. I quantify the impact of the time-consuming settlement process in the market for Bitcoin on arbitrageurs activity. The estimation rests on a novel threshold error correction model that exploits the notion that arbitrageurs suspend trading activity when arbitrage costs exceed price differences. I estimate substantial arbitrage costs that explain 63\\% of the observed price differences, where more than 75\\% of these costs can be attributed to the settlement process. I also find that a 10 bp decrease in latency-related arbitrage costs simultaneously results in a 3 bp increase of the quoted bid-ask spreads. I reconcile this finding in a theoretical model in which liquidity providers set larger spreads to cope with high adverse selection risks imposed by increased arbitrage activity. Consequently, efforts to reduce the latency of blockchain-based settlement might have unintended consequences for liquidity provision. In markets with substantial adverse selection risk, faster settlement may even harm price informativeness.","tags":[],"title":"Liquidity and Price Informativeness in Blockchain-Based Markets","type":"publication"},{"authors":["Albert J. Menkveld et al"],"categories":[],"content":"","date":1642291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642291200,"objectID":"48075de2a7c91c0d0982a1827a7f2ad3","permalink":"https://voigtstefan.me/publication/preprint/non-standard-errors/","publishdate":"2022-01-01T15:43:57+01:00","relpermalink":"/publication/preprint/non-standard-errors/","section":"publication","summary":"In statistics, samples are drawn from a population in a data-generating process (DGP). Standard errors measure the uncertainty in sample estimates of population parameters. In science, evidence is generated to test hypotheses in an evidence-generating process (EGP). We claim that EGP variation across researchers adds uncertainty: non-standard errors. To study them, we let 164 teams test six hypotheses on the same sample. We find that non-standard errors are sizeable, on par with standard errors. Their size (i) co-varies only weakly with team merits, reproducibility, or peer rating, (ii) declines significantly after peer-feedback, and (iii) is underestimated by participants.","tags":[],"title":"Non-standard Errors","type":"publication"},{"authors":["Doron Avramov","Si Cheng","Lior Metzker","Stefan Voigt"],"categories":[],"content":"","date":1642118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642118400,"objectID":"966bbeee765610ebe7c9d19e8ff03141","permalink":"https://voigtstefan.me/publication/journal-article/integrated-factor-models/","publishdate":"2021-11-09T15:38:53+01:00","relpermalink":"/publication/journal-article/integrated-factor-models/","section":"publication","summary":"This paper develops a comprehensive framework to address uncertainty about the correct factor model. Asset pricing inferences draw on a composite model that integrates over competing factor models weighted by posterior probabilities. Evidence shows that unconditional models record zero probabilities, and post-earnings announcement drift, quality-minus-junk, and intermediary capital are incremental factors in conditional asset pricing. The integrated model tilts away from the subsequently underperforming factors, and delivers stable and admissible strategies. Model uncertainty makes equities appear considerably riskier, while model disagreement about expected returns spikes during crash episodes. Disagreement spans all return components involving mispricing, factor loadings, and risk premia.","tags":[],"title":"Integrating Factor Models","type":"publication"},{"authors":[],"categories":[],"content":"We are pleased to announce that the “Vienna–Copenhagen Conference on Financial Econometrics (VieCo)” takes place June 2-4, 2022 in Copenhagen. It is jointly organized by the Department of Economics of the University of Copenhagen and the Department of Statistics and Operations Research of the University of Vienna. The aim of the conference is to bring together leading experts and practitioners in financial econometrics, financial statistics, quantitative financial economics as well as applied mathematical finance.\nConference homepage: https://eventsignup.ku.dk/vieco2022. Confirmed Keynote Speakers 2022\n Blockchain Technologies in Finance: Bruno Biais, HEC Paris Financial Econometrics: Olivier Scaillet, GSEM, Université de Genève Bootstrap Methods in Finance: Giuseppe Cavaliere, University of Bologna  Paper submission\nWe invite submissions within financial econometrics, financial statistics, quantitative financial economics and applied mathematical finance. Please submit your paper in .pdf format via our submission page. Deadline for submission of papers is February 1, 2022.\nYou will be given notice of acceptance via e-mail not later than March 15, 2022.\nWe look forward to your submission and to see you in person in CPH soon!\nBest,\nAnders Rahbek, Nikolaus Hautsch, Rasmus Søndergaard Pedersen and Stefan Voigt (Conference Organizers)\n","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640015797,"objectID":"e5922177ce700a8e06eb2472ab6a9bfb","permalink":"https://voigtstefan.me/post/2021-12-21-vieco2022/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/post/2021-12-21-vieco2022/","section":"post","summary":"We are pleased to announce that the “Vienna–Copenhagen Conference on Financial Econometrics (VieCo)” takes place June 2-4, 2022 in Copenhagen. It is jointly organized by the Department of Economics of the University of Copenhagen and the Department of Statistics and Operations Research of the University of Vienna.","tags":[],"title":"Call for Papers VieCo 2022 (Please circulate!)","type":"post"},{"authors":[],"categories":[],"content":"I regularly supervise empirical bachelor projects or master thesis in Finance on the following topics: Machine Learning Applications in Asset Pricing, Optimal Portfolio Choice problems or Microstructure of Decentralized Exchanges.\nDo you share my enthusiasm about any of these fields? Excellent, let us get in touch!\nIf you would like me to supervise your thesis, do not hesitate to reach out to me after you read the information below. To make the supervision process as smooth as possible for you, I follow the following general structure and enforce few rules:\n  Before we get started: Drop me a mail with a maximum 1-page description of your proposed topic. You should state a first research question and a reference to one (not more, not less!) published academic journal – which you are excited about and which is closest and most relevant for your own project (preferably from one of the top 3 finance journals: The Journal of Finance, Review of Financial Studies or Journal of Financial Economics). Provide a brief outline how you want to study the question, if you already have access to required data and highlight any problems that could arise along the way – the earlier we discuss potential problems, the easier the upcoming months will become.\n  If I do have supervision capacities, I will provide you structured feedback on your desired project such that you can start as smooth as possible. You can use my feedback to update your project description and to generate a preliminary roadmap (which indicates precise dates on which you plan to finish the individual steps). This roadmap is part of the supervision contract.\n  During the semester: I leave it as your responsibility to reach out by mail when you feel that a meeting is most meaningful – it is natural that sometime there are many important changes, sometimes you will just need time for yourself to work on the project. I do not enforce a strict timeline of meetings but rather let you sign up for meetings (up to 4 for Master students, up to 2 for Bachelor students) via mail – let me know at least 8 working days in advance that you want to meet.\n  Before a supervision meeting: I enforce a clear structure for the supervision meetings, which requires self-assessment on an ongoing basis. Latest 48 hours before the supervision meetings you will hand in your current manuscript and a self-assessment (max 1 page) on what you think are your most relevant questions (max. 3) and your own perception of the status, including an updated version of your intended timeline. These notes are your reflections on where you are in the process, what you feel comfortable about, where you are in doubt. I will base my supervision on these comments. Note: Without such a document and the manuscript as preparation in due time, I will cancel the meeting without replacement as I will not be able to give you high-level feedback without appropriate preparation material.\n  During the supervision meeting: I will read your documents beforehand and provide feedback regarding my general assessment, the specific questions you highlighted in your preparation and my suggestions regarding the most critical tasks going forward. If necessary, you will receive a mail with my annotations on your manuscript after the meeting such that we do not have to waste time discussing on grammar, typos or formatting issues during the meeting.\n  Latest 48 hours after the meeting: Send me an updated roadmap – I will sign it off and you are ready to proceed.\n  I look forward to hear about your definitely exciting research project from you as soon as possible!\n\rwindow.onload = function() { Calendly.initBadgeWidget({ url: 'https://calendly.com/voigtstefan', text: 'Supervision meeting needed? Arrange here!', color: '#ff1600', textColor: '#ffffff', branding: true }); }\r","date":1637798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637855797,"objectID":"4d413505532c6dc43c14ce619db8ff72","permalink":"https://voigtstefan.me/post/supervision/","publishdate":"2021-11-25T00:00:00Z","relpermalink":"/post/supervision/","section":"post","summary":"I regularly supervise empirical bachelor projects or master thesis in Finance on the following topics: Machine Learning Applications in Asset Pricing, Optimal Portfolio Choice problems or Microstructure of Decentralized Exchanges.","tags":[],"title":"Thesis supervisor needed? Get in touch!","type":"post"},{"authors":["Stefan Voigt"],"categories":[],"content":"I recently stumbled uppon the amazing Connect Kaggle Competition and I tried to improve my humble knowledge on reinforcement learning by participating in this challenge.\nThe task Very simple: Write an agent that plays  Connect Four against competing algorithms. My way to tackle it: \u0026ldquo;Deep\u0026rdquo; Q-Learning Sure, I could write some deterministic code on how to proceed in the game but what I actually implement is a seemingly brute-force method: Let the agent play the game over and over again and learn the rules the hard way. More specifically, the agents receives information on the current observation (the current state of the board) and then has to take an action (which slot to choose to add a coin). After that, nature responses with a new state and potentially yields a reward (if the game is won) or a penalty (if the game is lost or if the agent chooses an action that is not valid - such as putting a coin into an already full slot).\nHow should the agent decide on her action? In finance, the concept of dynamic programming, more specifically, the Bellman-equation, is well-known: Aim at actions that yield the highest expected reward. You can do so, by value each (action, state) pair with respect to the immediate rewards and the transition into the next_state. More specifically, you value an action $a$ given the current state $s_t$ as\n$$Q(a, s_t) = r + \\gamma\\max\\limits_{a\u0026rsquo;}\\hat{Q}(a\u0026rsquo;, s_{t+1})$$\nwhere $\\gamma$ is a discount factor and $\\hat{Q}$ is the (predicted) value of the next state. If we\u0026rsquo;d play a simpler game, we could try to store all possible (action, state) pairs and compute the optimal action. However, Connect Four in its basic fashion has 4531985219092 different possible states, so good luck with that aproach (which would be called Q-Learning, by the way).\nWhat I do instead, is approximating this function using a Neural network, simply because I have always wanted to implement something like this. The python kernel below summarises my implementation and tremendously benefits from Hieu Phungs work on Q-Learning and Keon Kims blog.\nInitialization Below packages are setting up the environment. Kaggle provides an entire framework to test your agent. keras is using the TensorFlow backend to handle the neural network.\nimport gym\rimport numpy as np\rfrom math import exp, log\r#import random\rfrom random import choice, uniform\rfrom collections import deque\rfrom keras.models import Sequential\rfrom keras.layers import Dense\rfrom keras.optimizers import Adam\rfrom kaggle_environments import evaluate, make\r Using TensorFlow backend.\r Define Environment The ConnectX environment below allows to play around with the setup in a clean \u0026lsquo;gym\u0026rsquo; style which makes it very easy to interact with current states. In order to train my agent properly, the switch_side and switch_trainer functions are called whenever we start a new game. Therefore, the agent (hopefully) learns to play on both sides of the board against the provided negamax and the random opponent (random just drops coins into arbitrarily chosen slots). For the purpose of illustrating the code, I switch the negamax function of, however.\nclass ConnectX(gym.Env):\rdef __init__(self, switch_prob=0.5):\rself.env = make('connectx', debug=True)\rself.pair = [None, 'random']\rself.trainer = self.env.train(self.pair)\rself.switch_prob = switch_prob\rconfig = self.env.configuration\rself.action_space = gym.spaces.Discrete(config.columns)\rself.observation_space = gym.spaces.Box(low=0, high=2, shape=(config.rows,config.columns,1), dtype=np.int)\rdef switch_side(self):\rself.pair = self.pair[::-1]\rself.trainer = self.env.train(self.pair)\rdef switch_trainer(self):\rcurrent_trainer_random = 'random' in self.pair if current_trainer_random:\rself.pair = [None, 'negamax']\relse:\rself.pair = [None, 'random']\rself.trainer = self.env.train(self.pair)\rdef step(self, action):\rreturn self.trainer.step(action)\rdef reset(self):\rif random.uniform(0, 1) \u0026lt; self.switch_prob: # switch side\rself.switch_side()\r#if random.uniform(0, 1) \u0026lt; self.switch_prob: # switch trainer\r# self.switch_trainer() return self.trainer.reset()\r Deep Learning Agent I am really not an expert in neural nets. Thus, all I do is playing around a bit. The magic in defining the agent as below is happening in the replay function: After gathering some experience, a neural network is trained to make sense of the state, action and reward relationship. The target is set such that the network aims at minimizing the loss between predicting the reward of the next_state and the realized reward.\n# Deep Q-learning Agent\rclass DQNAgent:\rdef __init__(self, state_size, action_size, episodes):\rself.state_size = state_size\rself.action_size = action_size\rself.memory = deque(maxlen=500)\rself.gamma = 0.9 # discount rate\rself.epsilon = 0.10 # initial exploration rate\rself.epsilon_min = 0.01\rself.epsilon_decay = exp((log(self.epsilon_min) - log(self.epsilon))/(0.8*episodes)) # reaches epsilon_min after 80% of iterations\rself.model = self._build_model()\rdef _build_model(self):\r# Neural Net for Deep-Q learning Model\rmodel = Sequential()\rmodel.add(Dense(20, input_dim=self.state_size, activation='relu'))\rmodel.add(Dense(50, activation='relu'))\rmodel.add(Dense(self.action_size, activation='linear'))\rmodel.compile(loss='mse',\roptimizer=Adam(lr = 0.00001))\rreturn model\rdef memorize(self, state, action, reward, next_state, done):\rself.memory.append((state, action, reward, next_state, done))\rdef act(self, state):\rif np.random.rand() \u0026lt;= self.epsilon: # Exploration\rreturn choice([c for c in range(self.action_size) if state[:,c] == 0])\r#when exploring, I allow for \u0026quot;wrong\u0026quot; moves to give the agent a chance #to experience the penalty of choosing full columns\r#return choice([c for c in range(self.action_size)])\ract_values = self.model.predict(state) # Exploitation\raction = np.argmax(act_values[0]) return action\rdef replay(self, batch_size):\rminibatch = random.sample(self.memory, batch_size)\rfor state, action, reward, next_state, done in minibatch:\rtarget = reward\rif not done:\rtarget = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\rtarget_f = self.model.predict(state)\rtarget_f[0][action] = target\rself.model.fit(state, target_f, epochs=1, verbose=0)\rif self.epsilon \u0026gt; self.epsilon_min:\rself.epsilon *= self.epsilon_decay\rdef load(self, name):\rself.model.load_weights(name)\rdef save(self, name):\rself.model.save_weights(name)\r Train the agent Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration. Notable thing here is that I let the agent also learn what a valid move is the hard way (a move is invalid if the agent chooses a column which is already full). After an invalid move the game is over (done = True) and I penalize invalid actions hard.\n# initialize gym environment and the agent\renv = ConnectX(switch_prob = 0.5)\rstate_size = env.observation_space.shape[1]*env.observation_space.shape[0]\raction_size = env.observation_space.shape[1]\repisodes = 40000\ragent = DQNAgent(state_size, action_size, episodes)\ragent.load(\u0026quot;./connectX-weights_deep.h5\u0026quot;) # load prelearned weights\rbatch_size = 40 # Don't know if this number makes sense\r# Monitoring devices\rall_total_rewards = np.empty(episodes)\rall_avg_rewards = np.empty(episodes)\r# Iterate the game\rfor e in range(episodes):\r# reset state in the beginning of each game\rdone = False\rstate = env.reset()\rtotal_rewards = 0\rwhile not done:\r# Decide action\raction = int(agent.act(np.array([state.board])))\rnext_state, reward, done, _ = env.step(action)\rif not done:\rreward = 0.0/42 # default: reward of 0.5 if not done/ 1 if win/ 0 if lost\rif done:\rif reward == 1: # Won\rreward = 1\relif reward == 0: # Lost\rreward = -1\relse: # Draw\rreward = 0\rif state.board[action]!=0: # invalid move: hard penalization\rreward = -10\ragent.memorize(np.array([state.board]), action, reward, np.array([next_state.board]), done)\r# make next_state the new current state for the next frame.\rstate = next_state\rtotal_rewards += reward\rif len(agent.memory) \u0026gt; batch_size:\ragent.replay(batch_size)\rall_total_rewards[e] = total_rewards\ravg_reward = all_total_rewards[max(0, e - 100):e].mean()\rall_avg_rewards[e] = avg_reward\rif e % 100 == 0 :\ragent.save(\u0026quot;./connectX-weights_deep.h5\u0026quot;)\rprint(\u0026quot;episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\u0026quot;.format(e, episodes, agent.epsilon, avg_reward))\r After a lot of training (millions of iterations with a prescheduled decreasing learning rate), the agent seems to have learned quite a bit: although I do not prevent the agent from choosing invalid actions, after some time such events basically do not happen anymore. Further, the agent starts winning against the random opponent with a fast increasing frequency.\nepisode: 39100/40000, epsilon: 0.01, average: 0.66\rInvalid Action: Invalid column: 1\repisode: 39200/40000, epsilon: 0.01, average: 0.65\repisode: 39300/40000, epsilon: 0.01, average: 0.64\repisode: 39400/40000, epsilon: 0.01, average: 0.78\repisode: 39500/40000, epsilon: 0.01, average: 0.70\repisode: 39600/40000, epsilon: 0.01, average: 0.68\repisode: 39700/40000, epsilon: 0.01, average: 0.72\repisode: 39800/40000, epsilon: 0.01, average: 0.68\repisode: 39900/40000, epsilon: 0.01, average: 0.64\r Did the agent learn anything? The learned weights are used to compute actions of the agent during the games. The figure below shows the average rewards gained by the trained agent (including the penalty for chosing invalid actions).\nimport matplotlib.pyplot as plt\rplt.plot(all_avg_rewards)\rplt.xlabel('Episode')\rplt.ylabel('Avg rewards (100)')\rplt.show()\r Finally, the real-life test: Submission of the agent to Kaggle. The procedure is somewhat cumbersome procedure because Kaggle does not allow keras modules for submission but the below procedure seems to work\nmodel = Sequential()\rmodel = Sequential()\rmodel.add(Dense(20, input_dim=state_size, activation='relu'))\rmodel.add(Dense(50, activation='relu'))\rmodel.add(Dense(action_size, activation='linear'))\rmodel.load_weights('connectX-weights_deep.h5')\rlayers = []\r# Get all layers' weights\rfor i in range(3):\rweights, biases = model.layers[i].get_weights()\rlayers.extend([weights, biases])\rfc_layers = list(map(\rlambda x: str(list(np.round(x, 8))) \\\r.replace('array(', '').replace(')', '') \\\r.replace(' ', '') \\\r.replace('\\n', '') \\\r.replace(',dtype=float32',''),\rlayers\r))\rfc_layers = np.reshape(fc_layers, (-1, 2))\r# Create the agent\rmy_agent = '''def my_agent(observation, configuration):\rimport numpy as np\r'''\r# Write hidden layers\rfor i, (w, b) in enumerate(fc_layers[:-1]):\rmy_agent += ' hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\rmy_agent += ' hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\rmy_agent += ' ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\rmy_agent += ' ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\rmy_agent += '''\rstate = observation.board[:]\r# state.append(observation.mark)\rout = np.array(state, dtype=np.float32)\r'''\rfor i in range(len(fc_layers[:-1])):\rmy_agent += ' out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\rmy_agent += ' out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\rmy_agent += ' out = np.matmul(out, ol_w) + ol_b\\n'\rmy_agent += '''\rfor i in range(configuration.columns):\rif observation.board[i] != 0:\rout[i] = -1e7\rreturn int(np.argmax(out))\r'''\rwith open('submission.py', 'w') as f:\rf.write(my_agent)\r Yes, she did! from submission import my_agent\renv = make(\u0026quot;connectx\u0026quot;, debug=True)\renv.run([my_agent, my_agent])\rprint(\u0026quot;Success!\u0026quot; if env.state[0].status == env.state[1].status == \u0026quot;DONE\u0026quot; else \u0026quot;Failed...\u0026quot;)\rdef mean_reward(rewards):\rreturn sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\r# Run multiple episodes to estimate agent's performance.\rprint(\u0026quot;My Agent vs. Random Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [my_agent, \u0026quot;random\u0026quot;], num_episodes=50)))\rprint(\u0026quot;Random Agent vs. My Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [\u0026quot;random\u0026quot;, my_agent], num_episodes=50)))\r#print(\u0026quot;My Agent vs. Negamax Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [my_agent, \u0026quot;negamax\u0026quot;], num_episodes=10)))\r#print(\u0026quot;Negamax Agent vs. My Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [\u0026quot;negamax\u0026quot;, my_agent], num_episodes=10)))\r So it seems that Deep-Q-Learning helped: by just playing against an random agent, the neural network was trained to win the game - even without knowing the rules in advance!\nMy Agent vs. Random Agent: 0.88\rRandom Agent vs. My Agent: 0.24\r ","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586857476,"objectID":"4aaf3a22679ecbd0b7dd20ce5d8522cf","permalink":"https://voigtstefan.me/post/connectx/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/connectx/","section":"post","summary":"Bellman Equation outside of Finance. A short report on my first Kaggle Competition","tags":[],"title":"Connect Four - Deep Reinforcement Learning","type":"post"},{"authors":["Stefan Voigt"],"categories":[],"content":"\rAfter the warm-up in the last post, I’ll move to a more serious analysis of data from Lobster.\n\rShina App Iframe\r\r\rDownloading and extracting massive data\rFirst, I requested all orderbook messages from Lobster up to level 10 from June 2007 until March 2020. During that period, SPY trading was very active: I observe more than 4.26 billion messages. Total trading volume of SPY on NASDAQ during that period exceeded 5.35 Trillion USD.\nLobster compiles the data on request and provides downloadable .7z files after processing the messages. To download everything (on a Linux machine), it is advisable to make use of wget (you’ll have to replace username, password and user_id with your own credentials):\nwget -bqc -P lobster_raw ftp://username:password@lobsterdata.com/user_id/*\rAs a next step, extract the .7z files before working with the individual files - although it is possible to read in the files from within the zipped folder, I made the experience that this can cause problems when done in parallel.\n7z e .lobster_raw/SPY_2019-06-27_2020-03-26_10.7z -o./data/lobster\r7z e .lobster_raw/SPY_2018-06-27_2019-06-26_10.7z -o./data/lobster\r7z e .lobster_raw/SPY_2017-06-27_2018-06-26_10.7z -o./data/lobster\r....\r3208 trading days occupy roughly 3.2 Terabyte of hard drive space. As explained in my previous post, I compute summary statistics for each single day in my sample. For the sake of brevity, the code snippet below is everything needed to do this in a straightforward parallel fashion using Slurm Workload Manager (the actual task 01_summarise_lobster_messages.R can be downloaded here).\n#$ -N lobster_summary\r#$ -t 1:3208\r#$ -e SPY_Investigation/Chunk\r#$ -o SPY_Investigation/Chunk\rR-g --vanilla \u0026lt; SPY_Investigation/01_summarise_lobster_messages.R\r\rMerge and summarise\rNext, I merge and evaluate the resulting files.\n# Required packages\rlibrary(tidyverse)\rlibrary(lubridate)\r# Asset and Date information\rasset \u0026lt;- \u0026quot;SPY\u0026quot;\rexisting_files \u0026lt;- dir(pattern=paste0(\u0026quot;LOBSTER_\u0026quot;, asset, \u0026quot;.*_summary.csv\u0026quot;), path=\u0026quot;output/summary_files\u0026quot;,\rfull.names = TRUE)\rsummary_data \u0026lt;- map(existing_files, function(x)\r{read_csv(x, col_names = TRUE, cols(ts_minute = col_datetime(format = \u0026quot;\u0026quot;),\rmidquote = col_double(),\rspread = col_double(),\rvolume = col_double(),\rhidden_volume = col_double(),\rdepth_bid = col_double(),\rdepth_ask = col_double(),\rdepth_bid_5 = col_double(),\rdepth_ask_5 = col_double(),\rmessages = col_double()))})\rsummary_data \u0026lt;- summary_data %\u0026gt;% bind_rows()\rwrite_csv(summary_data, paste0(\u0026quot;output/LOBSTER_\u0026quot;,asset,\u0026quot;_summary.csv\u0026quot;))\r\rSPY Depth is at an all-time low\rIn their paper Bid Price Dispersion, Albert Menkveld and Boyan Jovanovic document (among many other interesting things) a striking downwards trend in depth of the orderbook of SPY, the most actively traded ETF in the world.\ndata_by_date \u0026lt;- data %\u0026gt;% mutate (date = ymd(floor_date(ts_minute, \u0026quot;day\u0026quot;))) %\u0026gt;%\rgroup_by(date) %\u0026gt;% select(-ts_minute) %\u0026gt;% summarise_all(median)\rFeel free to play around with the Shiny Gadget at the beginning of the post to convince yourself: We see a negative trend in quoted spreads (apart from a couple of outliers) but as the figure below simultaneously shows, quoted depth at the best level as well as 5 basis points apart from the concurrent midquote decreased as well - note the extreme drop in liquidity provisioning since the beginning of 2020. The red line in the figure shows the daily average number of shares at the best ask (blue line corresponds to the bid). The dashed lines correspond to depth at 5 basis points (the number of shares available within 5 basis points from the midquote). Note that the y-axis is in a log scale, thus the figure hints at much more mass of the depth around the best levels.\r\rCOVID19 and the SP500\rNeedless to say, COVID19 caused turbulent days for global financial markets. The figure below illustrates how quoted liquidity and trading activity changed since January 13th, 2020, the first day WHO reported a case outside of China. More specifically, I plot the intra-daily dynamics of some of the derived measures for the entire year 2019 and the last couple of weeks.\ncorona_threshold \u0026lt;- \u0026quot;2020-01-13\u0026quot;\rbin_data \u0026lt;- data %\u0026gt;% mutate(\rbin = ymd_hms(cut(ts_minute, \u0026quot;5 min\u0026quot;)),\rbin = strftime(bin, format=\u0026quot;%H:%M:%S\u0026quot;),\rbin = as.POSIXct(bin, format=\u0026quot;%H:%M:%S\u0026quot;)) %\u0026gt;%\rselect(bin, everything()) %\u0026gt;% filter(ts_minute \u0026gt; \u0026quot;01-01-2019\u0026quot;,\r(hour(bin)\u0026gt;\u0026quot;09\u0026quot; \u0026amp; minute(bin)\u0026gt;\u0026quot;35\u0026quot;) | (hour(bin)\u0026lt;=\u0026quot;15\u0026quot; \u0026amp; minute(bin)\u0026lt;\u0026quot;55\u0026quot;)) %\u0026gt;% group_by(bin, Corona = ts_minute\u0026gt;=corona_threshold) %\u0026gt;% summarise_all(list(mean=mean)) \r\rShina App Iframe\r\r\r\r","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585940843,"objectID":"455cc56953017821e457965eb8332906","permalink":"https://voigtstefan.me/post/lobster-large-scale-liquidity-analysis/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/lobster-large-scale-liquidity-analysis/","section":"post","summary":"A short  series of posts on handling high-frequency data from Lobster and R","tags":[],"title":"LobsteR - Analysing a Decade of High-Frequency Trading","type":"post"},{"authors":["Stefan Voigt"],"categories":[],"content":"\rDuring my PhD studies, I have been working with high-frequency trading data provided by Lobster a lot for some of my research projects.\nIn this short series of posts, I want share some of my code and routines to efficiently handle the extremely large amounts of data that go through NASDAQs servers on a daily basis. In fact, if you look at the figure below, there is plenty to explore: during less than 2 minutes on March 17th, 2020, thousands of trades have been executed for SPY, a large ETF. The red line shows the traded prices during that period and the blue shaded areas show the dynamics of the orderbook. The darker the areas, the more liquidity (measured as size of the order book levels).\nFirst, I provide some snippets to read-in Lobster files and to compute some potentially interesting statistics. In a second post, I illustrate long-run characteristics of the orderbook dynamics and I’ll finally focus some really recent events: the days since the outbreak of COVID19 have been extremely bumpy for SPY, the largest ETF in the world and it is amazing to see, how liquidity supply changed during these rough days.\nHandling Lobster Data\rLobster is an online limit order book data tool to provide easy-to-use, high-quality limit order book data for the entire universe of NASDAQ traded stocks. I requested some of the data based on their online interface and stored it before running the code below.\rThe actual data which I will use for the next post is much larger. I downloaded all trading messages for ticker SPY (order submissions, cancellations, trades, …) that went through NASDAQ since July, 27th 2007 until March, 25th, 2020. The files contain the entire orderbooks until level 10.\nFirst steps\rI work in R with message level data from Lobster in a tidy and (hopefully) efficient way.\nlibrary(tidyverse)\rlibrary(lubridate)\rAs an example, I illustrate the computations for a tiny glimpse of March 17th, 2020. Lobster files always come with the same naming convention ticker_date_34200000_57600000_filetype_level.csv, whereas filetype either denotes message or the corresponding orderbook snapshots.\nasset \u0026lt;- \u0026quot;SPY\u0026quot;\rdate \u0026lt;- \u0026quot;2020-03-17\u0026quot;\rlevel \u0026lt;- 10\rmessages_filename \u0026lt;- paste0(asset,\u0026quot;_\u0026quot;,date,\u0026quot;_34200000_57600000_message_\u0026quot;, level,\u0026quot;.csv\u0026quot;)\rorderbook_filename \u0026lt;- paste0(asset, \u0026quot;_\u0026quot;,date,\u0026quot;_34200000_57600000_orderbook_\u0026quot;, level,\u0026quot;.csv\u0026quot;)\rLet’s have a look at the raw message feed first.\nmessages_raw \u0026lt;- read_csv(messages_filename, col_names = c(\u0026quot;ts\u0026quot;, \u0026quot;type\u0026quot;, \u0026quot;order_id\u0026quot;, \u0026quot;m_size\u0026quot;, \u0026quot;m_price\u0026quot;, \u0026quot;direction\u0026quot;, \u0026quot;null\u0026quot;),\rcol_types = cols(ts = col_double(), type = col_integer(),\rorder_id = col_integer(),\rm_size = col_double(),\rm_price = col_double(),\rdirection = col_integer(),\rnull = col_skip())) %\u0026gt;% mutate(ts = as.POSIXct(ts, origin=date, tz=\u0026quot;GMT\u0026quot;), m_price = m_price / 10000)\rmessages_raw\r## # A tibble: 20,000 x 6\r## ts type order_id m_size m_price direction\r## \u0026lt;dttm\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 2020-03-17 09:30:00 4 24654260 230 245 1\r## 2 2020-03-17 09:30:00 3 24683304 500 245. -1\r## 3 2020-03-17 09:30:00 3 24690848 500 245. 1\r## 4 2020-03-17 09:30:00 1 24699256 500 245. -1\r## 5 2020-03-17 09:30:00 3 24690812 500 245. -1\r## 6 2020-03-17 09:30:00 3 24699256 500 245. -1\r## 7 2020-03-17 09:30:00 1 24699992 500 245. 1\r## 8 2020-03-17 09:30:00 1 24700384 500 245. 1\r## 9 2020-03-17 09:30:00 3 24700384 500 245. 1\r## 10 2020-03-17 09:30:00 1 24700516 500 245. 1\r## # ... with 19,990 more rows\rBy default, ts denotes the time in seconds since midnight (decimals are precise until nanosecond level) and price always comes in 10.000 USD. type denotes the message type: 4, for instance, corresponds to the execution of a visible order. The remaining variables are explained in more detail here.\nNext, the corresponding orderbook snapshots contain price and quoted size for each of the 10 levels.\norderbook_raw \u0026lt;- read_csv(orderbook_filename,\rcol_names = paste(rep(c(\u0026quot;ask_price\u0026quot;, \u0026quot;ask_size\u0026quot;, \u0026quot;bid_price\u0026quot;, \u0026quot;bid_size\u0026quot;), level),\rrep(1:level, each=4), sep=\u0026quot;_\u0026quot;),\rcols(.default = col_double())) %\u0026gt;% mutate_at(vars(contains(\u0026quot;price\u0026quot;)), ~./10000)\r\rPutting the files together\rEach message is associated with the corresponding orderbook snapshot at that point in time.\rAfter merging message and orderbook files, the entire data thus looks as follows\norderbook \u0026lt;- bind_cols(messages_raw, orderbook_raw) \r\r\rts\rtype\rorder_id\rm_size\rm_price\rask_price_1\rask_size_1\rbid_price_1\rbid_size_1\r\r\r\r2020-03-17 09:30:00\r4\r24654260\r230\r245.00\r245.10\r500\r244.88\r1000\r\r2020-03-17 09:30:00\r3\r24683304\r500\r245.14\r245.10\r500\r244.88\r1000\r\r2020-03-17 09:30:00\r3\r24690848\r500\r244.88\r245.10\r500\r244.88\r500\r\r2020-03-17 09:30:00\r1\r24699256\r500\r245.03\r245.03\r500\r244.88\r500\r\r2020-03-17 09:30:00\r3\r24690812\r500\r245.10\r245.03\r500\r244.88\r500\r\r2020-03-17 09:30:00\r3\r24699256\r500\r245.03\r245.11\r500\r244.88\r500\r\r\r\r\r\rCompute summary statistics\rNext, I compute summary statistics on 20 second levels. In particular I am interested in quoted prices, spreads, and depth (the amount of tradeable units in the orderbook):\n\rMidquote \\(q_t = (a_t + b_t)/2\\) (where \\(a_t\\) and \\(b_t\\) denote the best bid and best ask)\rSpread \\(S_t= (a_t - b_t)\\) (values below are computed in basis points relative to the concurrent midquote)\rVolume is the aggretate sum of traded units of the stock. I do differentiate between hidden (type==5) and visible volume.\r\rorderbook \u0026lt;- orderbook %\u0026gt;% mutate(midquote = ask_price_1/2 + bid_price_1/2, spread = (ask_price_1 - bid_price_1)/midquote * 10000,\rvolume = if_else(type ==4|type ==5, m_size, 0),\rhidden_volume = if_else(type ==5, m_size, 0))\rAs a last step, depth of the orderbook denotes the number of assets that can be traded without moving the quoted price more than a given range (measured in basis points) from the concurrent midquote. The function below takes care of the slightly involved computations.\ncompute_depth \u0026lt;- function(df, side = \u0026quot;bid\u0026quot;, bp = 0){\rif(side ==\u0026quot;bid\u0026quot;){\rvalue_bid \u0026lt;- (1-bp/10000)*df %\u0026gt;% select(\u0026quot;bid_price_1\u0026quot;) index_bid \u0026lt;- df %\u0026gt;% select(contains(\u0026quot;bid_price\u0026quot;)) %\u0026gt;% mutate_all(function(x) {x \u0026gt;= value_bid})\rsum_vector \u0026lt;- (df %\u0026gt;% select(contains(\u0026quot;bid_size\u0026quot;))*index_bid) %\u0026gt;% rowSums()\r}else{\rvalue_ask \u0026lt;- (1+bp/10000)*df %\u0026gt;% select(\u0026quot;ask_price_1\u0026quot;)\rindex_ask \u0026lt;- df %\u0026gt;% select(contains(\u0026quot;ask_price\u0026quot;)) %\u0026gt;% mutate_all(function(x) {x \u0026lt;= value_ask})\rsum_vector \u0026lt;- (df %\u0026gt;% select(contains(\u0026quot;ask_size\u0026quot;))*index_ask) %\u0026gt;% rowSums()\r}\rreturn(sum_vector)\r}\rorderbook \u0026lt;- orderbook %\u0026gt;% mutate(depth_bid = compute_depth(orderbook),\rdepth_ask = compute_depth(orderbook, side=\u0026quot;ask\u0026quot;),\rdepth_bid_5 = compute_depth(orderbook, bp = 5),\rdepth_ask_5 = compute_depth(orderbook, bp = 5, side=\u0026quot;ask\u0026quot;))\rAlmost there! The snippet below splits the data into 20 second intervals and computes the averages of the computed summary statistics.\norderbook_dense \u0026lt;- orderbook %\u0026gt;%\rmutate(ts_minute = floor_date(ts, \u0026quot;20 seconds\u0026quot;)) %\u0026gt;% select(midquote:ts_minute) %\u0026gt;% group_by(ts_minute) %\u0026gt;% mutate(messages = n(),\rvolume = sum(volume),\rhidden_volume = sum(hidden_volume)) %\u0026gt;%\rsummarise_all(mean)\rHere we go: during the first 100 seconds on March 17th, 20.000 messages related to the orderbook of SPY have been processed by NASDAQ. The quoted spread on average was around 3bp. On average, roughly 90.000 contracts have been traded during each 20 second slot - in other words, assets worth roughly 90 million USD have been exchanged. Quoted liquidity at the best bid and best ask seems rather small relative to the tremendous amounts of trading activity during this (very short) period of time.\n\r\r\r\rts_minute\rmidquote\rspread\rvolume\rhidden_volume\rdepth_bid\rdepth_ask\rdepth_bid_5\rdepth_ask_5\rmessages\r\r\r\r2020-03-17 09:30:00\r245.0332\r4.010257\r89606\r19923\r353.7358\r354.1362\r1854.152\r2516.916\r5890\r\r2020-03-17 09:30:20\r245.2229\r3.142070\r54733\r23716\r190.3232\r238.8164\r2099.857\r2041.646\r3165\r\r2020-03-17 09:30:40\r245.5052\r2.177630\r53273\r18188\r121.9574\r182.5553\r2113.945\r2282.149\r4246\r\r2020-03-17 09:31:00\r245.2010\r1.488751\r146974\r86780\r297.4000\r254.3316\r1985.406\r2416.603\r4210\r\r2020-03-17 09:31:20\r244.6590\r1.514445\r26286\r6655\r122.6870\r115.6107\r2174.080\r2325.517\r2489\r\r\r\rFinally, some visualisation of the data at hand: The code below creates the figure at the beginning of the post and shows the dynamics of the traded prices (red line) and the quoted prices at the higher levels of the orderbook.\norderbook_trades \u0026lt;- orderbook %\u0026gt;% filter(type==4|type==5) %\u0026gt;% select(ts, m_price)\rorderbook_quotes \u0026lt;- orderbook %\u0026gt;% mutate(id = row_number()) %\u0026gt;%\rselect(ts, id, matches(\u0026quot;bid|ask\u0026quot;)) %\u0026gt;% gather(level, price, -ts, -id) %\u0026gt;%\rseparate(level, into=c(\u0026quot;side\u0026quot;,\u0026quot;variable\u0026quot;,\u0026quot;level\u0026quot;), sep=\u0026quot;_\u0026quot;) %\u0026gt;%\rmutate(level = as.numeric(level)) %\u0026gt;% spread(variable, price)\rp1 \u0026lt;- ggplot() + theme_bw() +\rgeom_point(data = orderbook_quotes, aes(x=ts, y=price, color=level, size = size/max(size)), alpha = 0.1)+\rgeom_line(data = orderbook_trades, aes(x=ts, y=m_price), color=\u0026#39;red\u0026#39;) + labs(title=\u0026quot;SPY: Orderbook Dynamics\u0026quot;,\ry=\u0026quot;Price\u0026quot;,\rx=\u0026quot;\u0026quot;) +\rtheme(panel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(),\rlegend.position =\u0026quot;none\u0026quot;) +\rscale_y_continuous()\r\r","date":1585094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585297296,"objectID":"2a759b70efd04c772da2747c308e2260","permalink":"https://voigtstefan.me/post/lobster-1/","publishdate":"2020-03-25T00:00:00Z","relpermalink":"/post/lobster-1/","section":"post","summary":"A short  series of posts on handling high-frequency data from Lobster and R","tags":[],"title":"LobsteR - NASDAQ under a \"tidy\" Microscope","type":"post"},{"authors":["Stefan Voigt","Nikolaus Hautsch"],"categories":[],"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b476c6af7959a702c2871aa0d4c64ccd","permalink":"https://voigtstefan.me/publication/journal-article/large-scale-portfolio-optimization-under-transaction-costs-and-model-uncertainty/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/publication/journal-article/large-scale-portfolio-optimization-under-transaction-costs-and-model-uncertainty/","section":"publication","summary":"We theoretically and empirically study portfolio optimization under transaction costs and establish a link between turnover penalization and covariance shrinkage with the penalization governed by transaction costs. We show how the ex ante incorporation of transaction costs shifts optimal portfolios towards regularized versions of efficient allocations. The regulatory effect of transaction costs is studied in an econometric setting incorporating parameter uncertainty and optimally combining predictive distributions resulting from high-frequency and low-frequency data. In an extensive empirical study, we illustrate that turnover penalization is more effective than commonly employed shrinkage methods and is crucial in order to construct empirically well-performing portfolios.","tags":[],"title":"Large Scale Portfolio Optimization under Transaction Costs and Model Uncertainty","type":"publication"}]