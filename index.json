[{"authors":["admin"],"categories":null,"content":"I am tenure-track assistant professor of Finance at the Department of Economics at the University in Copenhagen.\nI have a deep interest in the economic implications and evolution of blockchain-based settlement in financial markets. I pursue research questions related to market fragmentation, high frequency trading and big data in financial applications. My research is thus anchored in the intersection of market microstructure, asset pricing and financial econometrics.\nYou can find my current research on SSRN and arXiv.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am tenure-track assistant professor of Finance at the Department of Economics at the University in Copenhagen.\nI have a deep interest in the economic implications and evolution of blockchain-based settlement in financial markets.","tags":null,"title":"","type":"authors"},{"authors":["Stefan Voigt"],"categories":[],"content":"","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589846400,"objectID":"f70f2bc58c28548ce81c9907b3bcd93f","permalink":"/publication/liquidity-and-price-informativeness-in-blockchain-based-markets/","publishdate":"2019-03-20T15:43:57+01:00","relpermalink":"/publication/liquidity-and-price-informativeness-in-blockchain-based-markets/","section":"publication","summary":"Blockchain-based markets impose substantial costs on cross-market trading due to the decentralized and time-consuming settlement process. I quantify the impact of the time-consuming settlement process in the market for Bitcoin on arbitrageurs activity. The estimation rests on a novel threshold error correction model that exploits the notion that arbitrageurs suspend trading activity when arbitrage costs exceed price differences. I estimate substantial arbitrage costs that explain 63\\% of the observed price differences, where more than 75\\% of these costs can be attributed to the settlement process. I also find that a 10 bp decrease in latency-related arbitrage costs simultaneously results in a 3 bp increase of the quoted bid-ask spreads. I reconcile this finding in a theoretical model in which liquidity providers set larger spreads to cope with high adverse selection risks imposed by increased arbitrage activity. Consequently, efforts to reduce the latency of blockchain-based settlement might have unintended consequences for liquidity provision. In markets with substantial adverse selection risk, faster settlement may even harm price informativeness.","tags":[],"title":"Liquidity and Price Informativeness in Blockchain-Based Markets","type":"publication"},{"authors":["Stefan Voigt"],"categories":[],"content":"I recently stumbled uppon the amazing Connect Kaggle Competition and I tried to improve my humble knowledge on reinforcement learning by participating in this challenge.\nThe task Very simple: Write an agent that plays  Connect Four against competing algorithms. My way to tackle it: \u0026ldquo;Deep\u0026rdquo; Q-Learning Sure, I could write some deterministic code on how to proceed in the game but what I actually implement is a seemingly brute-force method: Let the agent play the game over and over again and learn the rules the hard way. More specifically, the agents receives information on the current observation (the current state of the board) and then has to take an action (which slot to choose to add a coin). After that, nature responses with a new state and potentially yields a reward (if the game is won) or a penalty (if the game is lost or if the agent chooses an action that is not valid - such as putting a coin into an already full slot).\nHow should the agent decide on her action? In finance, the concept of dynamic programming, more specifically, the Bellman-equation, is well-known: Aim at actions that yield the highest expected reward. You can do so, by value each (action, state) pair with respect to the immediate rewards and the transition into the next_state. More specifically, you value an action $a$ given the current state $s_t$ as\n$$Q(a, s_t) = r + \\gamma\\max\\limits_{a\u0026rsquo;}\\hat{Q}(a\u0026rsquo;, s_{t+1})$$\nwhere $\\gamma$ is a discount factor and $\\hat{Q}$ is the (predicted) value of the next state. If we\u0026rsquo;d play a simpler game, we could try to store all possible (action, state) pairs and compute the optimal action. However, Connect Four in its basic fashion has 4531985219092 different possible states, so good luck with that aproach (which would be called Q-Learning, by the way).\nWhat I do instead, is approximating this function using a Neural network, simply because I have always wanted to implement something like this. The python kernel below summarises my implementation and tremendously benefits from Hieu Phungs work on Q-Learning and Keon Kims blog.\nInitialization Below packages are setting up the environment. Kaggle provides an entire framework to test your agent. keras is using the TensorFlow backend to handle the neural network.\nimport gym\rimport numpy as np\rfrom math import exp, log\r#import random\rfrom random import choice, uniform\rfrom collections import deque\rfrom keras.models import Sequential\rfrom keras.layers import Dense\rfrom keras.optimizers import Adam\rfrom kaggle_environments import evaluate, make\r Using TensorFlow backend.\r Define Environment The ConnectX environment below allows to play around with the setup in a clean \u0026lsquo;gym\u0026rsquo; style which makes it very easy to interact with current states. In order to train my agent properly, the switch_side and switch_trainer functions are called whenever we start a new game. Therefore, the agent (hopefully) learns to play on both sides of the board against the provided negamax and the random opponent (random just drops coins into arbitrarily chosen slots). For the purpose of illustrating the code, I switch the negamax function of, however.\nclass ConnectX(gym.Env):\rdef __init__(self, switch_prob=0.5):\rself.env = make('connectx', debug=True)\rself.pair = [None, 'random']\rself.trainer = self.env.train(self.pair)\rself.switch_prob = switch_prob\rconfig = self.env.configuration\rself.action_space = gym.spaces.Discrete(config.columns)\rself.observation_space = gym.spaces.Box(low=0, high=2, shape=(config.rows,config.columns,1), dtype=np.int)\rdef switch_side(self):\rself.pair = self.pair[::-1]\rself.trainer = self.env.train(self.pair)\rdef switch_trainer(self):\rcurrent_trainer_random = 'random' in self.pair if current_trainer_random:\rself.pair = [None, 'negamax']\relse:\rself.pair = [None, 'random']\rself.trainer = self.env.train(self.pair)\rdef step(self, action):\rreturn self.trainer.step(action)\rdef reset(self):\rif random.uniform(0, 1) \u0026lt; self.switch_prob: # switch side\rself.switch_side()\r#if random.uniform(0, 1) \u0026lt; self.switch_prob: # switch trainer\r# self.switch_trainer() return self.trainer.reset()\r Deep Learning Agent I am really not an expert in neural nets. Thus, all I do is playing around a bit. The magic in defining the agent as below is happening in the replay function: After gathering some experience, a neural network is trained to make sense of the state, action and reward relationship. The target is set such that the network aims at minimizing the loss between predicting the reward of the next_state and the realized reward.\n# Deep Q-learning Agent\rclass DQNAgent:\rdef __init__(self, state_size, action_size, episodes):\rself.state_size = state_size\rself.action_size = action_size\rself.memory = deque(maxlen=500)\rself.gamma = 0.9 # discount rate\rself.epsilon = 0.10 # initial exploration rate\rself.epsilon_min = 0.01\rself.epsilon_decay = exp((log(self.epsilon_min) - log(self.epsilon))/(0.8*episodes)) # reaches epsilon_min after 80% of iterations\rself.model = self._build_model()\rdef _build_model(self):\r# Neural Net for Deep-Q learning Model\rmodel = Sequential()\rmodel.add(Dense(20, input_dim=self.state_size, activation='relu'))\rmodel.add(Dense(50, activation='relu'))\rmodel.add(Dense(self.action_size, activation='linear'))\rmodel.compile(loss='mse',\roptimizer=Adam(lr = 0.00001))\rreturn model\rdef memorize(self, state, action, reward, next_state, done):\rself.memory.append((state, action, reward, next_state, done))\rdef act(self, state):\rif np.random.rand() \u0026lt;= self.epsilon: # Exploration\rreturn choice([c for c in range(self.action_size) if state[:,c] == 0])\r#when exploring, I allow for \u0026quot;wrong\u0026quot; moves to give the agent a chance #to experience the penalty of choosing full columns\r#return choice([c for c in range(self.action_size)])\ract_values = self.model.predict(state) # Exploitation\raction = np.argmax(act_values[0]) return action\rdef replay(self, batch_size):\rminibatch = random.sample(self.memory, batch_size)\rfor state, action, reward, next_state, done in minibatch:\rtarget = reward\rif not done:\rtarget = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\rtarget_f = self.model.predict(state)\rtarget_f[0][action] = target\rself.model.fit(state, target_f, epochs=1, verbose=0)\rif self.epsilon \u0026gt; self.epsilon_min:\rself.epsilon *= self.epsilon_decay\rdef load(self, name):\rself.model.load_weights(name)\rdef save(self, name):\rself.model.save_weights(name)\r Train the agent Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration. Notable thing here is that I let the agent also learn what a valid move is the hard way (a move is invalid if the agent chooses a column which is already full). After an invalid move the game is over (done = True) and I penalize invalid actions hard.\n# initialize gym environment and the agent\renv = ConnectX(switch_prob = 0.5)\rstate_size = env.observation_space.shape[1]*env.observation_space.shape[0]\raction_size = env.observation_space.shape[1]\repisodes = 40000\ragent = DQNAgent(state_size, action_size, episodes)\ragent.load(\u0026quot;./connectX-weights_deep.h5\u0026quot;) # load prelearned weights\rbatch_size = 40 # Don't know if this number makes sense\r# Monitoring devices\rall_total_rewards = np.empty(episodes)\rall_avg_rewards = np.empty(episodes)\r# Iterate the game\rfor e in range(episodes):\r# reset state in the beginning of each game\rdone = False\rstate = env.reset()\rtotal_rewards = 0\rwhile not done:\r# Decide action\raction = int(agent.act(np.array([state.board])))\rnext_state, reward, done, _ = env.step(action)\rif not done:\rreward = 0.0/42 # default: reward of 0.5 if not done/ 1 if win/ 0 if lost\rif done:\rif reward == 1: # Won\rreward = 1\relif reward == 0: # Lost\rreward = -1\relse: # Draw\rreward = 0\rif state.board[action]!=0: # invalid move: hard penalization\rreward = -10\ragent.memorize(np.array([state.board]), action, reward, np.array([next_state.board]), done)\r# make next_state the new current state for the next frame.\rstate = next_state\rtotal_rewards += reward\rif len(agent.memory) \u0026gt; batch_size:\ragent.replay(batch_size)\rall_total_rewards[e] = total_rewards\ravg_reward = all_total_rewards[max(0, e - 100):e].mean()\rall_avg_rewards[e] = avg_reward\rif e % 100 == 0 :\ragent.save(\u0026quot;./connectX-weights_deep.h5\u0026quot;)\rprint(\u0026quot;episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\u0026quot;.format(e, episodes, agent.epsilon, avg_reward))\r After a lot of training (millions of iterations with a prescheduled decreasing learning rate), the agent seems to have learned quite a bit: although I do not prevent the agent from choosing invalid actions, after some time such events basically do not happen anymore. Further, the agent starts winning against the random opponent with a fast increasing frequency.\nepisode: 39100/40000, epsilon: 0.01, average: 0.66\rInvalid Action: Invalid column: 1\repisode: 39200/40000, epsilon: 0.01, average: 0.65\repisode: 39300/40000, epsilon: 0.01, average: 0.64\repisode: 39400/40000, epsilon: 0.01, average: 0.78\repisode: 39500/40000, epsilon: 0.01, average: 0.70\repisode: 39600/40000, epsilon: 0.01, average: 0.68\repisode: 39700/40000, epsilon: 0.01, average: 0.72\repisode: 39800/40000, epsilon: 0.01, average: 0.68\repisode: 39900/40000, epsilon: 0.01, average: 0.64\r Did the agent learn anything? The learned weights are used to compute actions of the agent during the games. The figure below shows the average rewards gained by the trained agent (including the penalty for chosing invalid actions).\nimport matplotlib.pyplot as plt\rplt.plot(all_avg_rewards)\rplt.xlabel('Episode')\rplt.ylabel('Avg rewards (100)')\rplt.show()\r Finally, the real-life test: Submission of the agent to Kaggle. The procedure is somewhat cumbersome procedure because Kaggle does not allow keras modules for submission but the below procedure seems to work\nmodel = Sequential()\rmodel = Sequential()\rmodel.add(Dense(20, input_dim=state_size, activation='relu'))\rmodel.add(Dense(50, activation='relu'))\rmodel.add(Dense(action_size, activation='linear'))\rmodel.load_weights('connectX-weights_deep.h5')\rlayers = []\r# Get all layers' weights\rfor i in range(3):\rweights, biases = model.layers[i].get_weights()\rlayers.extend([weights, biases])\rfc_layers = list(map(\rlambda x: str(list(np.round(x, 8))) \\\r.replace('array(', '').replace(')', '') \\\r.replace(' ', '') \\\r.replace('\\n', '') \\\r.replace(',dtype=float32',''),\rlayers\r))\rfc_layers = np.reshape(fc_layers, (-1, 2))\r# Create the agent\rmy_agent = '''def my_agent(observation, configuration):\rimport numpy as np\r'''\r# Write hidden layers\rfor i, (w, b) in enumerate(fc_layers[:-1]):\rmy_agent += ' hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\rmy_agent += ' hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\rmy_agent += ' ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\rmy_agent += ' ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\rmy_agent += '''\rstate = observation.board[:]\r# state.append(observation.mark)\rout = np.array(state, dtype=np.float32)\r'''\rfor i in range(len(fc_layers[:-1])):\rmy_agent += ' out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\rmy_agent += ' out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\rmy_agent += ' out = np.matmul(out, ol_w) + ol_b\\n'\rmy_agent += '''\rfor i in range(configuration.columns):\rif observation.board[i] != 0:\rout[i] = -1e7\rreturn int(np.argmax(out))\r'''\rwith open('submission.py', 'w') as f:\rf.write(my_agent)\r Yes, she did! from submission import my_agent\renv = make(\u0026quot;connectx\u0026quot;, debug=True)\renv.run([my_agent, my_agent])\rprint(\u0026quot;Success!\u0026quot; if env.state[0].status == env.state[1].status == \u0026quot;DONE\u0026quot; else \u0026quot;Failed...\u0026quot;)\rdef mean_reward(rewards):\rreturn sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\r# Run multiple episodes to estimate agent's performance.\rprint(\u0026quot;My Agent vs. Random Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [my_agent, \u0026quot;random\u0026quot;], num_episodes=50)))\rprint(\u0026quot;Random Agent vs. My Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [\u0026quot;random\u0026quot;, my_agent], num_episodes=50)))\r#print(\u0026quot;My Agent vs. Negamax Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [my_agent, \u0026quot;negamax\u0026quot;], num_episodes=10)))\r#print(\u0026quot;Negamax Agent vs. My Agent:\u0026quot;, mean_reward(evaluate(\u0026quot;connectx\u0026quot;, [\u0026quot;negamax\u0026quot;, my_agent], num_episodes=10)))\r So it seems that Deep-Q-Learning helped: by just playing against an random agent, the neural network was trained to win the game - even without knowing the rules in advance!\nMy Agent vs. Random Agent: 0.88\rRandom Agent vs. My Agent: 0.24\r ","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586857476,"objectID":"4aaf3a22679ecbd0b7dd20ce5d8522cf","permalink":"/post/connectx/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/connectx/","section":"post","summary":"Bellman Equation outside of Finance. A short report on my first Kaggle Competition","tags":[],"title":"Connect Four - Deep Reinforcement Learning","type":"post"},{"authors":["Stefan Voigt"],"categories":[],"content":"\rAfter the warm-up in the last post, I’ll move to a more serious analysis of data from Lobster.\n\rShina App Iframe\r\r\rDownloading and extracting massive data\rFirst, I requested all orderbook messages from Lobster up to level 10 from June 2007 until March 2020. During that period, SPY trading was very active: I observe more than 4.26 billion messages. Total trading volume of SPY on NASDAQ during that period exceeded 5.35 Trillion USD.\nLobster compiles the data on request and provides downloadable .7z files after processing the messages. To download everything (on a Linux machine), it is advisable to make use of wget (you’ll have to replace username, password and user_id with your own credentials):\nwget -bqc -P lobster_raw ftp://username:password@lobsterdata.com/user_id/*\rAs a next step, extract the .7z files before working with the individual files - although it is possible to read in the files from within the zipped folder, I made the experience that this can cause problems when done in parallel.\n7z e .lobster_raw/SPY_2019-06-27_2020-03-26_10.7z -o./data/lobster\r7z e .lobster_raw/SPY_2018-06-27_2019-06-26_10.7z -o./data/lobster\r7z e .lobster_raw/SPY_2017-06-27_2018-06-26_10.7z -o./data/lobster\r....\r3208 trading days occupy roughly 3.2 Terabyte of hard drive space. As explained in my previous post, I compute summary statistics for each single day in my sample. For the sake of brevity, the code snippet below is everything needed to do this in a straightforward parallel fashion using Slurm Workload Manager (the actual task 01_summarise_lobster_messages.R can be downloaded here).\n#$ -N lobster_summary\r#$ -t 1:3208\r#$ -e SPY_Investigation/Chunk\r#$ -o SPY_Investigation/Chunk\rR-g --vanilla \u0026lt; SPY_Investigation/01_summarise_lobster_messages.R\r\rMerge and summarise\rNext, I merge and evaluate the resulting files.\n# Required packages\rlibrary(tidyverse)\rlibrary(lubridate)\r# Asset and Date information\rasset \u0026lt;- \u0026quot;SPY\u0026quot;\rexisting_files \u0026lt;- dir(pattern=paste0(\u0026quot;LOBSTER_\u0026quot;, asset, \u0026quot;.*_summary.csv\u0026quot;), path=\u0026quot;output/summary_files\u0026quot;,\rfull.names = TRUE)\rsummary_data \u0026lt;- map(existing_files, function(x)\r{read_csv(x, col_names = TRUE, cols(ts_minute = col_datetime(format = \u0026quot;\u0026quot;),\rmidquote = col_double(),\rspread = col_double(),\rvolume = col_double(),\rhidden_volume = col_double(),\rdepth_bid = col_double(),\rdepth_ask = col_double(),\rdepth_bid_5 = col_double(),\rdepth_ask_5 = col_double(),\rmessages = col_double()))})\rsummary_data \u0026lt;- summary_data %\u0026gt;% bind_rows()\rwrite_csv(summary_data, paste0(\u0026quot;output/LOBSTER_\u0026quot;,asset,\u0026quot;_summary.csv\u0026quot;))\r\rSPY Depth is at an all-time low\rIn their paper Bid Price Dispersion, Albert Menkveld and Boyan Jovanovic document (among many other interesting things) a striking downwards trend in depth of the orderbook of SPY, the most actively traded ETF in the world.\ndata_by_date \u0026lt;- data %\u0026gt;% mutate (date = ymd(floor_date(ts_minute, \u0026quot;day\u0026quot;))) %\u0026gt;%\rgroup_by(date) %\u0026gt;% select(-ts_minute) %\u0026gt;% summarise_all(median)\rFeel free to play around with the Shiny Gadget at the beginning of the post to convince yourself: We see a negative trend in quoted spreads (apart from a couple of outliers) but as the figure below simultaneously shows, quoted depth at the best level as well as 5 basis points apart from the concurrent midquote decreased as well - note the extreme drop in liquidity provisioning since the beginning of 2020. The red line in the figure shows the daily average number of shares at the best ask (blue line corresponds to the bid). The dashed lines correspond to depth at 5 basis points (the number of shares available within 5 basis points from the midquote). Note that the y-axis is in a log scale, thus the figure hints at much more mass of the depth around the best levels.\r\rCOVID19 and the SP500\rNeedless to say, COVID19 caused turbulent days for global financial markets. The figure below illustrates how quoted liquidity and trading activity changed since January 13th, 2020, the first day WHO reported a case outside of China. More specifically, I plot the intra-daily dynamics of some of the derived measures for the entire year 2019 and the last couple of weeks.\ncorona_threshold \u0026lt;- \u0026quot;2020-01-13\u0026quot;\rbin_data \u0026lt;- data %\u0026gt;% mutate(\rbin = ymd_hms(cut(ts_minute, \u0026quot;5 min\u0026quot;)),\rbin = strftime(bin, format=\u0026quot;%H:%M:%S\u0026quot;),\rbin = as.POSIXct(bin, format=\u0026quot;%H:%M:%S\u0026quot;)) %\u0026gt;%\rselect(bin, everything()) %\u0026gt;% filter(ts_minute \u0026gt; \u0026quot;01-01-2019\u0026quot;,\r(hour(bin)\u0026gt;\u0026quot;09\u0026quot; \u0026amp; minute(bin)\u0026gt;\u0026quot;35\u0026quot;) | (hour(bin)\u0026lt;=\u0026quot;15\u0026quot; \u0026amp; minute(bin)\u0026lt;\u0026quot;55\u0026quot;)) %\u0026gt;% group_by(bin, Corona = ts_minute\u0026gt;=corona_threshold) %\u0026gt;% summarise_all(list(mean=mean)) \r\rShina App Iframe\r\r\r\r","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585940843,"objectID":"455cc56953017821e457965eb8332906","permalink":"/post/lobster-large-scale-liquidity-analysis/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/lobster-large-scale-liquidity-analysis/","section":"post","summary":"A short  series of posts on handling high-frequency data from Lobster and R","tags":[],"title":"LobsteR - Analysing a Decade of High-Frequency Trading","type":"post"},{"authors":["Nikolaus Hautsch","Christoph Scheuch","Stefan Voigt"],"categories":[],"content":"This paper replaces an earlier draft titled \u0026ldquo;Limits to Arbitrage in Markets with Stochastic Settlement Latency\u0026rdquo;.\n","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"918fffb669ad3e0ef949c3db8ebe0735","permalink":"/publication/trust-takes-time-limits-to-arbitrage-in-blockchain-based-markets/","publishdate":"2020-03-19T15:38:53+01:00","relpermalink":"/publication/trust-takes-time-limits-to-arbitrage-in-blockchain-based-markets/","section":"publication","summary":"Distributed ledger technologies replace central counterparties with time-consuming consensus protocols to record the transfer of ownership. This settlement latency slows down cross-market trading and exposes arbitrageurs to price risk. We theoretically derive arbitrage bounds induced by settlement latency. Using Bitcoin orderbook and network data, we estimate average arbitrage bounds of 121 basis points, explaining 91% of the cross-market price differences, and demonstrate that asset flows chase arbitrage opportunities. Controlling for inventory holdings as a measure of trust in exchanges does not affect our main results. Blockchain-based settlement without trusted intermediation thus introduces a non-trivial friction that impedes arbitrage activity.","tags":[],"title":"Building Trust Takes Time: Limits to Arbitrage in Blockchain-Based Markets","type":"publication"},{"authors":["Stefan Voigt","Nikolaus Hautsch"],"categories":[],"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b45959558b23aae2294c2bbc2a8b1db9","permalink":"/publication/large-scale-portfolio-optimization-under-transaction-costs-and-model-uncertainty/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/publication/large-scale-portfolio-optimization-under-transaction-costs-and-model-uncertainty/","section":"publication","summary":"We theoretically and empirically study portfolio optimization under transaction costs and establish a link between turnover penalization and covariance shrinkage with the penalization governed by transaction costs. We show how the ex ante incorporation of transaction costs shifts optimal portfolios towards regularized versions of efficient allocations. The regulatory effect of transaction costs is studied in an econometric setting incorporating parameter uncertainty and optimally combining predictive distributions resulting from high-frequency and low-frequency data. In an extensive empirical study, we illustrate that turnover penalization is more effective than commonly employed shrinkage methods and is crucial in order to construct empirically well-performing portfolios.","tags":[],"title":"Large Scale Portfolio Optimization under Transaction Costs and Model Uncertainty","type":"publication"}]